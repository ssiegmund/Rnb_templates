---
title: "discover cluster for template data"
author: "Sascha Siegmund"
date: "`r Sys.Date()`"
output: 
  github_document:
    fig_width: 10
    fig_height: 7
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  fig.path = "nb_figs/clus_",
  warning = FALSE,
  message = FALSE
)
```



## purpose of notebook

  - [ ] 
  
## insights 

  - 


## load packages

```{r, setup, include=FALSE}
library(tidyverse) # tidy data frame
library(plotly) # make ggplots interactive

library(factoextra) # extract and visualize the output of multivariate data analyses

# individual libraries are in the according cell
```


## import data

```{r, message=FALSE}
df <- read_csv('../data/heart_attack_classification/heart.csv')
```


## overview

```{r}
head(df)
```
```{r}
summary(df)
```

## config: set number of cluster

```{r}
no_k = 2 # number of clusters for clustering methods 
max_k = 7 # maximum clusters in scope 
min_k = 2 # minimum clusters in scope
```


## prepare data

```{r}
# remove variables which are highly correlated or dichtome or not wanted in analysis
value_df <- df %>% select(everything()) %>% 
  na.omit()

# list with columns to exclude
var = c('Education', 'Marital_Status')

# replace data with scaled columns
 # scale_df <- as_tibble(scale(df[, !names(df) %in% cat]))
preObj <- caret::preProcess(value_df[, !names(value_df) %in% var], method=c("center", "scale"))
scale_df <- predict(preObj, value_df[, !names(value_df) %in% var])
head(scale_df)

# find more methods here: https://stackoverflow.com/questions/15215457/standardize-data-columns-in-r
```


## identify best clustering approach and optimal number of clusters

- 

```{r}
tmp_df <- scale_df %>% as.matrix()

library(clValid) # simultaneously compares multiple clustering algorithms, 
                 # to identify the best clustering approach and the optimal number of clusters
library(mclust) # BIC for parameterized Gaussian mixture models fitted by EM algorithm 

results <- clValid(tmp_df, nClust = min_k:max_k, 
                  clMethods = c("hierarchical", "kmeans", "diana", "fanny", "som", 
                                "model", "sota", "pam", "clara", "agnes"),
                  validation = c("internal", "stability"))

summary(results)
```

```{r}
library(NbClust) # determining the optimal number of clusters in a data set
library(cluster) # computes various cluster algorithms

fviz_nbclust(scale_df, kmeans, method = 'wss') + # Elbow method
  labs(subtitle = "Elbow method") # add subtitle

fviz_nbclust(scale_df, kmeans, method = 'silhouette') + # Silhouette method
  labs(subtitle = "Silhouette method") # add subtitle

fviz_nbclust(scale_df, kmeans, # Gap statistics
             nstart = 30,
             method = 'gap_stat',
             nboot = 100) + # reduce it for lower computation time, but less precise results
  labs(subtitle = "Gap statistics method")

nbclust_out <- NbClust(data = scale_df,
                       distance = 'euclidean',
                       min.nc = min_k, # minimum number of clusters
                       max.nc = max_k, # maximum number of cluster
                       method = 'complete',
                       index = 'all')
fviz_nbclust(nbclust_out) + theme_minimal() + 
  labs(subtitle = "NbClust results")
```


```{r, fig.width = 15, fig.height = 9}
library(clustree) # produce clustering trees, a visualization to interrogate clusterings as resolution increase

tmp <- NULL
for (k in 1:max_k){
  tmp[k] <- kmeans(scale_df, k, nstart = 30) # kmeans used to produce clusters
}

tmp_df <- data.frame(tmp)
colnames(tmp_df) <- seq(1:max_k) # add prefix to the column names
colnames(tmp_df) <- paste0("k", colnames(tmp_df)) 

# get individual PCA
tmp_df.pca <- prcomp(tmp_df, center = TRUE, scale. = FALSE)

ind.coord <- tmp_df.pca$x
ind.coord <- ind.coord[,1:2]

tmp_df <- bind_cols(as.data.frame(tmp_df), as.data.frame(ind.coord))

clustree(tmp_df, prefix = "k") # produce clustering trees, a visualization for interrogating clustering 

overlay_list <- clustree_overlay(tmp_df, prefix = "k", x_value = "PC1", y_value = "PC2", plot_sides = TRUE)
overlay_list$overlay
overlay_list$x_side
overlay_list$y_side
```


## hierarchical clustering

- 

```{r}
# Hierarchical clustering: single linkage
hclust_res <- hclust(dist(scale_df), method = 'single')
plot(hclust_res)
rect.hclust(hclust_res, k = no_k, border = 'blue')
# fviz_dend(hclust_res, k = no_k, rect = TRUE) # can take some time for big data sets
 
# Hierarchical clustering: complete linkage
hclust_res <- hclust(dist(scale_df), method = 'complete')
plot(hclust_res)
rect.hclust(hclust_res, k = no_k, border = 'blue')
# fviz_dend(hclust_res, k = no_k, rect = TRUE) 

# Hierarchical clustering: average linkage
hclust_res <- hclust(dist(scale_df), method = 'average')
plot(hclust_res)
rect.hclust(hclust_res, k = no_k, border = 'blue')
# fviz_dend(hclust_res, k = no_k, rect = TRUE) 

# Hierarchical clustering: ward
hclust_res <- hclust(dist(scale_df), method = 'ward.D2')
# plot(hclust_res)
# rect.hclust(hclust_res, k = no_k, border = 'blue')
fviz_dend(hclust_res, k = no_k, rect = TRUE)

# Hierarchical clustering: mcquitty
hclust_res <- hclust(dist(scale_df), method = 'mcquitty')
plot(hclust_res)
rect.hclust(hclust_res, k = no_k, border = 'blue')
# fviz_dend(hclust_res, k = no_k, rect = TRUE) 

# Hierarchical clustering: centroid
hclust_res <- hclust(dist(scale_df), method = 'centroid')
plot(hclust_res)
rect.hclust(hclust_res, k = no_k, border = 'blue')
# fviz_dend(hclust_res, k = no_k, rect = TRUE) 
```


## k-means clustering

- 

```{r}
# k-means clustering, centers corresponds to the number of desired clusters
kmeans_model <- kmeans(scale_df, centers = no_k, nstart = 30) 

# store cluster in original data set as column
df_cluster <- tibble(value_df, cluster = as.factor(kmeans_model$cluster)) 

# check quality of a k-means partition
quality <- kmeans_model$betweenss / kmeans_model$totss 
print(paste("quality of kmeans is BSS/TSS: ", format(round(quality,2), nsmall = 2)))

# check quality of clustering
#  if a large majority of the silhouette coefficients are positive, 
#  it indicates that the observations are placed in the correct group
sil <- silhouette(kmeans_model$cluster, dist(scale_df)) 
fviz_silhouette(sil)

fviz_cluster(kmeans_model, scale_df, ellipse.type = 'norm') + theme_minimal()
fviz_cluster(kmeans_model, scale_df) + theme_minimal()

# trying multiple cluster numbers 
kmean_calc <- function(df, ...){
  kmeans(df, scaled = ..., nstart = 30)
}

km2 <- kmean_calc(scale_df, 2)
km3 <- kmean_calc(scale_df, 3)
km4 <- kmeans(scale_df, 4)
km5 <- kmeans(scale_df, 5)
km6 <- kmeans(scale_df, 6)
km7 <- kmeans(scale_df, 7)

p1 <- fviz_cluster(km2, data = scale_df, ellipse.type = "convex") + theme_minimal()
p1 <- ggplotly(p1) %>% layout(annotations = list(text = "k = 2", xref = "paper", yref = "paper", 
                                                 yanchor = "bottom", xanchor = "center", 
                                                 align = "center", x = 0.5, y = 1, showarrow = FALSE))
p2 <- fviz_cluster(km3, data = scale_df, ellipse.type = "convex") + theme_minimal()
p2 <- ggplotly(p2) %>% layout(annotations = list(text = "k = 3", xref = "paper", yref = "paper", 
                                                 yanchor = "bottom", xanchor = "center", 
                                                 align = "center", x = 0.5, y = 1, showarrow = FALSE))
p3 <- fviz_cluster(km4, data = scale_df, ellipse.type = "convex") + theme_minimal()
p3 <- ggplotly(p3) %>% layout(annotations = list(text = "k = 4", xref = "paper", yref = "paper", 
                                                 yanchor = "bottom", xanchor = "center", 
                                                 align = "center", x = 0.5, y = 1, showarrow = FALSE))
p4 <- fviz_cluster(km5, data = scale_df, ellipse.type = "convex") + theme_minimal()
p4 <- ggplotly(p4) %>% layout(annotations = list(text = "k = 5", xref = "paper", yref = "paper", 
                                                 yanchor = "bottom", xanchor = "center", 
                                                 align = "center", x = 0.5, y = 1, showarrow = FALSE))
p5 <- fviz_cluster(km6, data = scale_df, ellipse.type = "convex") + theme_minimal()
p5 <- ggplotly(p5) %>% layout(annotations = list(text = "k = 6", xref = "paper", yref = "paper", 
                                                 yanchor = "bottom", xanchor = "center", 
                                                 align = "center", x = 0.5, y = 1, showarrow = FALSE))
p6 <- fviz_cluster(km7, data = scale_df, ellipse.type = "convex") + theme_minimal()
p6 <- ggplotly(p6) %>% layout(annotations = list(text = "k = 7", xref = "paper", yref = "paper", 
                                                 yanchor = "bottom", xanchor = "center", 
                                                 align = "center", x = 0.5, y = 1, showarrow = FALSE))

fig <- subplot(p1, p2, p3 , p4, p5, p6, nrows = 2, shareX = TRUE, shareY = TRUE) %>% layout() 
fig
```


## principal component analysis + self organizing map cluster

- TODO: PCA should be in a dimensionality reduction notebook and only for viz shortly, SOM as a 

```{r}
library(kohonen) # functions to train self-organising maps (SOMs)

# principle component analysis
pca <- prcomp(scale_df, scale=FALSE)
summary(pca)

# visualize pcs results
# Contributions of variables to PC1
fviz_contrib(pca, choice = "var", axes = 1, top = 10)
# Contributions of variables to PC2
fviz_contrib(pca, choice = "var", axes = 2, top = 10)
# Control variable colors using their contributions to the principle axis
fviz_pca_var(pca, col.var="contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE # Avoid text overlapping
             ) + theme_minimal() + ggtitle("Variables - PCA")

# add back to original so everything is together
pca_scores <- data.frame(pca$x)
data_val <- cbind(value_df, pca_scores)

# clustering is performed using the som() function on the scaled gene expression values.
set.seed(3)

# define a grid for the SOM and train
grid_size <- ncol(scale_df)
som_grid <- somgrid(xdim = grid_size, ydim = grid_size, topo = 'hexagonal')
som_model <- som(as.matrix(scale_df), grid = som_grid)
summary(som_model)

# generate som plots after training
plot(som_model, type = 'mapping')
plot(som_model, type = 'codes')
# plot(som_model, type = 'counts')
# plot(som_model, type = 'dist.neighbours')
# plot(som_model, type = 'quality')
# plot(som_model, type = 'changes')

# further split the clusters into a smaller set of clusters using hierarchical clustering.
# use hierarchical clustering to cluster the codebook vectors
som_cluster <- cutree(hclust(dist(som_model$codes[[1]])), no_k) 

plot(som_model, type="mapping", bgcol = som_cluster, main = "Clusters")
add.cluster.boundaries(som_model, som_cluster)

# attach the hierchal cluster to the larger dataset data_val.
gridSquare <- grid_size * grid_size
som_clusterKey <- data.frame(som_cluster)
som_clusterKey$unit_classif <- c(1:gridSquare)
data_val <- cbind(data_val,som_model$unit.classif,som_model$distances) %>% 
  rename(unit_classif = 'som_model$unit.classif', distances = 'som_model$distances')
data_val <- merge(data_val, som_clusterKey, by.x = "unit_classif" )
head(data_val)

# plot pca with colored clusters
pcasom_plot <- ggplot(data_val, aes(x = PC1, y = PC2, color = factor(som_cluster))) +
    geom_rug(alpha = 0.5) + 
    geom_point(alpha = 0.75) + 
    theme_minimal()
pcasom_plot <- ggplotly(pcasom_plot) %>% layout()

pcasom_plot
```


## cluster validation

- take appropriate viz from notebook describe_group

```{r}
final <- kmeans(scale_df, no_k, nstart = 30)

as.tibble(scale_df) %>% 
  mutate(cluster = final$cluster) %>%
  group_by(cluster) %>%
  summarise_all('mean')

as.tibble(value_df) %>% 
  mutate(cluster = final$cluster) %>%
  group_by(cluster) %>%
  summarise_all('mean')

as.tibble(value_df) %>% 
  mutate(cluster = final$cluster) %>%
  group_by(cluster) %>%
  summarise_all('median')
```











